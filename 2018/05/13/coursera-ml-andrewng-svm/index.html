<html>
  <head>
    <title>吴恩达机器学习：支持向量机 - 算力喵</title>
    <link href='/images/fav.jpg' rel='shortcut icon'>
<link href='/atom.xml' rel='alternate' type='application/rss+xml'>
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight.css">
<link rel="stylesheet" href="/css/responsive.css">
<link rel="stylesheet" href="/css/iconfont.css">
<script src="/js/jquery.js"></script>
<script src="/js/basics.js"></script>
<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type'>

  <meta name="keywords" content="吴恩达,Coursera,SVM,支持向量机,机器学习,Machine Learning,算力喵">


  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-115469277-1');
  ga('send', 'pageview');
</script>



  <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  </head>
  <body>
    <header>
  <a id='go-back-home' href='/'><img class='avatar' src='/images/scribble.jpg' alt='Home'></a>
  <p id='title'>算力喵</p>
  <p>机器学习不能停~</p>
  <br>
  
  <div class="social-links">
    
      
        <a href="https://weibo.com/hertzcat" class="iconfont icon-weibo" title="weibo"></a>
      
    
      
        <a href="https://www.zhihu.com/people/hertzcat" class="iconfont icon-zhihu" title="zhihu"></a>
      
    
      
        <a href="https://twitter.com/hertzcat0" class="iconfont icon-twitter" title="twitter"></a>
      
    
  </div>
  
</header>

    <div id='container'>
      <div class='block'>
  
    <a class='main' href='/'>Blog</a>
  
    <a class='main' href='https://github.com/hertzcat'>Github</a>
  
</div>

      <section class='paging'>
  
  
    <div class='right'>
      <a href='/2018/04/27/tensorflow-playground/'>
        ›
      </a>
    </div>
  
</section>

      <div class='content'>
        <section class='post'>
          <h1>
            <!-- <div class='date'>2018-05-13</div> -->
            吴恩达机器学习：支持向量机
          </h1>
          <p>这次的课程笔记和上次隔了好久，因为为了搞懂 <strong>SVM</strong> 花了不少时间。和之前 <a href="http://hertzcat.com/2018/04/14/coursera-ml-andrewng-nn-back-propagation/"><strong>神经网络</strong></a> 课程一样，Ng 在 Coursera 上讲述的内容非常有限，要搞懂 SVM 你只能寻求于其他方面的资料。经过对网上一些内容的对比后发现斯坦福的 <a href="http://cs229.stanford.edu/syllabus.html" target="_blank" rel="noopener"><strong>CS229</strong></a> 讲义写得非常清晰。CS229 是 Andrew Ng 在斯坦福大学开设的机器学习课程，Coursera 上的课程可以说是 CS229 的简化版本，而且整个课程的内容还在不断更新，有兴趣的同学可以把它作为更进一步的参考资料。（ 我已经将讲义和 SMO 算法的论文与代码放在一起了方便大家下载 ）</p>
<p>这次的课程笔记会阐述 SVM 的主要思路，从 <strong>最大间距分类器</strong> 开始，然后通过 <strong>拉格朗日对偶</strong> 得到原问题的 <strong>对偶</strong> 问题，使得我们可以应用 <strong>核技巧（ kernel trick ）</strong>，用高效的方式解决高维问题。</p>
<p>点击 <a href="https://www.bilibili.com/video/av9912938?from=search&amp;seid=5092317489110083757" target="_blank" rel="noopener"><strong>课程视频</strong></a> 你就能不间断地学习 Ng 的课程，关于课程作业的 Python 代码已经放到了 Github 上，点击 <a href="https://github.com/hertzcat/Coursera-Machine-Learning" target="_blank" rel="noopener"><strong>课程代码</strong></a> 就能去 Github 查看（ 无法访问 Github 的话可以点击 <a href="https://coding.net/u/hertzcat/p/Coursera-Machine-Learning/git?public=true" target="_blank" rel="noopener"><strong>Coding</strong></a> 查看 ），代码中的错误和改进欢迎大家指出。</p>
<p>以下是 Ng 机器学习课程第六周的笔记。</p>
<h3 id="最大间距分类器"><a href="#最大间距分类器" class="headerlink" title="最大间距分类器"></a>最大间距分类器</h3><p>考虑 <strong>逻辑回归</strong> 的 <strong>预测函数</strong> <script type="math/tex">h_\theta(x)=g(\theta^Tx)</script>。当 <script type="math/tex">\theta^Tx\gg0</script> 或 <script type="math/tex">\theta^Tx\ll0</script> 时，就能十分确信 <strong>预测函数</strong> 的分类，因为 <script type="math/tex">h_\theta(x)</script> 接近于 1 和 0。直观的表达就是样本越是远离 <strong>决策边界</strong>，就越确信它的分类。所以自然想到如果在多个 <strong>决策边界</strong> 之间选择，我们会选择离所有样本都比较远的那个（ 考虑线性的情况 ）。<br><img src="/2018/05/13/coursera-ml-andrewng-svm/intuition.jpeg" title="[分类问题]"><br>为了得到这样的决策边界，我们首先来看看如何用数学方式表达这个问题。通过一些简单的 <strong>计算几何</strong> 知识可以计算出每个样本到 <strong>决策边界</strong> 的距离（ <strong>geometric margin</strong> ），<script type="math/tex">\gamma^{(i)}</script>。为了方便之后的推导，还需要定义一个 <strong>functional margin</strong>，<script type="math/tex">\hat{\gamma}^{(i)}</script>。</p>
<script type="math/tex; mode=display">
\hat{\gamma}^{(i)} = y^{(i)}\left(w^Tx^{(i)}+b\right) \\
\gamma^{(i)} = y^{(i)}\left(\left(\frac{w}{||w||}\right)^Tx^{(i)}+\frac{b}{||w||}\right)</script><p>在接下来的 <strong>SVM</strong> 讨论中 <script type="math/tex">y^{(i)}</script> 的取值为 <script type="math/tex">\{1,-1\}</script>（ 表示在决策边界的哪一侧，并使得距离的计算都是正的 ），当 <script type="math/tex">\theta^Tx\ge0</script> 时取 <script type="math/tex">1</script>，<script type="math/tex">\theta^Tx<0</script> 时取 <script type="math/tex">-1</script>。还需要定义 <script type="math/tex">\hat{\gamma}</script> 为所有 <script type="math/tex">\hat{\gamma}^{(i)}</script> 中最小的值，<script type="math/tex">\gamma</script> 为所有 <script type="math/tex">\gamma^{(i)}</script> 中最小的值。于是要求离所有样本都比较远的决策边界问题就成为了在给定约束条件下求最值的问题。</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
max_{\gamma,w,b} & \gamma \\
s.t. & y^{(i)}(w^Tx^{(i)}+b)\ge\gamma, & i=1,...,m \\
 & ||w||=1.
\end{array}</script><p>然而上面的问题还不是一个直接可解的 <a href="https://en.wikipedia.org/wiki/Optimization_problem" target="_blank" rel="noopener"><strong>优化问题</strong></a>，需要对它进行转化（ 思路是把它转化为标准的 <strong>凸优化问题</strong> ）。首先我们用 <strong>functional margin</strong> 来替换 <strong>geometric margin</strong>，原问题变为：</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
max_{\hat{\gamma},w,b} & \frac{\hat{\gamma}}{||w||} \\
s.t. & y^{(i)}(w^Tx^{(i)}+b)\ge\hat{\gamma}, & i=1,...,m
\end{array}</script><p>这个问题的解和原先的相同，不同点是在这个问题里，我们可以对 <script type="math/tex">w,b</script> 随意加倍而不用考虑 <script type="math/tex">||w||</script> 的大小了。为了剔除 <script type="math/tex">\hat{\gamma}</script> 项的干扰，我们适当地选取 <script type="math/tex">w,b</script> 的倍数使得 <script type="math/tex">\hat{\gamma}=1</script>，再加上最大化 <script type="math/tex">\frac{1}{||w||}</script> 相当于最小化 <script type="math/tex">||w||^2</script>，于是 <strong>优化问题</strong> 的最终形式可以化为：</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
min_{\gamma,w,b} & \frac{1}{2}||w||^2 \\
s.t. & y^{(i)}(w^Tx^{(i)}+b)\ge1, & i=1,...,m
\end{array}</script><p>对于这样的问题，已经可以用已知的 <strong>优化问题</strong> 算法来解决。不过对于 <strong>SVM</strong>，我们会考察一个与之相对应的问题。为了得到这个问题我们需要先了解下 <strong>拉格朗日对偶</strong>。</p>
<h3 id="拉格朗日对偶"><a href="#拉格朗日对偶" class="headerlink" title="拉格朗日对偶"></a>拉格朗日对偶</h3><p>对于 <strong>拉格朗日对偶</strong> 涉及的问题很多，具体细节我推荐大家看 C299 的讲义和 <a href="http://blog.pluskid.org/?p=702" target="_blank" rel="noopener"><strong>这篇博客</strong></a>。考虑一般的 <strong>凸优化问题</strong>：</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
min_{w} & f(w) \\
s.t. & g_{i}\le0, & i=1,...,k \\
 & h_{i}(w)=0, & i=1,...,l
\end{array}</script><p>设它的 <strong>拉格朗日函数</strong> 为：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w,\alpha,\beta)=f(w)+\sum\limits_{i=1}^{k}\alpha_ig_i(w)+\sum\limits_{i=1}^{l}\beta_ih_i(w)</script><p>考虑函数：</p>
<script type="math/tex; mode=display">
\theta_{\mathcal{P}}(w) = \max\limits_{\alpha,\beta:\alpha_{i}\ge0}\mathcal{L}(w,\alpha,\beta)</script><p>在 <script type="math/tex">w</script> 满足约束条件的时候 <script type="math/tex">\theta_{\mathcal{P}}(w)=f(w)</script>，在不满足时 <script type="math/tex">\theta_{\mathcal{P}}(w)=\infty</script>。所以下式中的 <script type="math/tex">p^*</script> 的结果与原问题相同。</p>
<script type="math/tex; mode=display">
p^*=\min\limits_{w}\theta_{\mathcal{P}}(w)</script><p>有意思的是对于这样的问题，总有一个与之对应问题（ <strong>对偶问题</strong> ），并且在特定条件下相同。要得到 <strong>对偶问题</strong>，我们只需要交换 <script type="math/tex">min,max</script> 的顺序，考虑函数：</p>
<script type="math/tex; mode=display">
\theta_{\mathcal{D}}(\alpha,\beta) = \min\limits_{w}\mathcal{L}(w,\alpha,\beta)</script><p><strong>对偶问题</strong> 的结果 <script type="math/tex">d^*</script> 满足下式，并且由不等式看出它是原问题的一个下界。</p>
<script type="math/tex; mode=display">
d^*=\max\limits_{\alpha,\beta:\alpha_{i}\ge0}\theta_{\mathcal{D}}(\alpha,\beta) \le \min\limits_{w}\theta_{\mathcal{P}}(w)=p^*</script><p>最后来看下 <strong>对偶问题</strong> 的解在什么情况下与 <strong>原问题</strong> 相同。如果函数 <script type="math/tex">f,g</script> 都是 <strong>凸函数</strong>，<script type="math/tex">h</script> 是 <strong>放射函数</strong>（ 线性的 ），并且存在 <script type="math/tex">w</script> 使得 <script type="math/tex">g_i(w)<0</script>，则优化问题有解 <script type="math/tex">w^*,\alpha^*,\beta^*</script>，并有 <script type="math/tex">d^*=p^*=\mathcal{L}(w^*,\alpha^*,\beta^*)</script>。 这时我们的解满足 <strong>KKT</strong> 条件，反之满足 <strong>KKT</strong> 条件的解也是优化问题的解（ <strong>KKT</strong> 条件如下 ）。 </p>
<blockquote>
<script type="math/tex; mode=display">\begin{array}{rl} \frac{\partial }{\partial w_i} \mathcal{L}(w^*,\alpha^*,\beta^*)=0, & i=1,...,m \end{array}</script><script type="math/tex; mode=display">\begin{array}{rl} \frac{\partial }{\partial \beta_i} \mathcal{L}(w^*,\alpha^*,\beta^*)=0, & i=1,...,l \end{array}</script><script type="math/tex; mode=display">\begin{array}{rl} \alpha_i^*g_i(w^*)=0, & i=1,...,k \end{array}</script><script type="math/tex; mode=display">\begin{array}{rl} g_i(w^*)\le0, & i=1,...,k \end{array}</script><script type="math/tex; mode=display">\begin{array}{rl} \alpha_i\ge0, & i=1,...,k \end{array}</script></blockquote>
<h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>通过 <strong>拉格朗日对偶</strong> 我们了解到之前的最大间距问题有一个与它对应的 <strong>对偶</strong> 问题。接下去我们就通过 <strong>拉格朗日对偶</strong> 来得到这个问题。<br>对于之前的问题令 <script type="math/tex">g_{i}(w)=-y^{(i)}(w^Tx^{(i)}+b)+1 \le0</script>，并设 <strong>拉格朗日函数</strong> 为：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w,b,\alpha)=\frac{1}{2}||w||^2-\sum\limits_{i=1}^{m}\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1]</script><p>根据对偶问题的定义，我们先对于 <script type="math/tex">w,b</script> 求 <script type="math/tex">\mathcal{L}(w,b,\alpha)</script> 的最小值，也就是分别对 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 进行求导得到：</p>
<script type="math/tex; mode=display">
w=\sum\limits_{i=1}^{m}\alpha_iy^{(i)}x^{(i)} \\
\sum\limits_{i=1}^{m}\alpha_iy^{(i)} = 0</script><p>将 <script type="math/tex">w</script> 带入 <script type="math/tex">\mathcal{L}(w,b,\alpha)</script> 进行化简得到：</p>
<script type="math/tex; mode=display">
\mathcal{L}(w,b,\alpha) = \sum\limits_{i=1}^{m}\alpha_i - \frac{1}{2} \sum\limits_{i,j=1}^{m}y^{(i)} y^{(j)} \alpha_i \alpha_j (x^{(i)})^T x^{(j)}</script><p>再加上 <script type="math/tex">\alpha_i\ge0</script> 与 <script type="math/tex">\sum\limits_{i=1}^{m}\alpha_iy^{(i)} = 0</script> 的约束条件，我们得到了最终的 <strong>对偶</strong> 问题：</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
max_{\alpha} & W(\alpha)=\sum\limits_{i=1}^{m}\alpha_i - \frac{1}{2} \sum\limits_{i,j=1}^{m}y^{(i)} y^{(j)} \alpha_i \alpha_j \color{red}{\langle x^{(i)},x^{(j)}\rangle} \\
s.t. & \alpha_i\ge0, \ \ \ \ \ i=1,...,m \\
 & \sum\limits_{i=1}^{m}\alpha_iy^{(i)} = 0
\end{array}</script><p>红色正是最重要的部分（ 尖括号表示内积 ），它使得我们可以运用 <strong>核函数</strong> 的技巧来降低计算的复杂度，特别是需要将特征映射到很高维甚至是无限维的情况。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>假设每个样本有三个特征 <script type="math/tex">x_1,x_2,x_3</script>，通常需要把它们映射到更高维来拟合更复杂的函数，让我们假设 <strong>映射函数</strong> 为：</p>
<script type="math/tex; mode=display">
\phi(x)= \begin{bmatrix}
    x_1x_1 \\
    x_1x_2 \\
    x_1x_3 \\
    x_2x_1 \\
    x_2x_2 \\
    x_2x_3 \\
    x_3x_1 \\
    x_3x_2 \\
    x_3x_3 \\
\end{bmatrix}</script><p>在映射后两个不同样本间的内积为：</p>
<script type="math/tex; mode=display">
\begin {aligned}
\langle \phi(x),\phi(z) \rangle & = \sum\limits_{i,j=1}^{3}(x_ix_j)(z_iz_j) \\
                                & = \sum\limits_{i=1}^{3}\sum\limits_{j=1}^{3}x_ix_jz_iz_j \\
                                & = (\sum\limits_{i=1}^{3}x_iz_i)(\sum\limits_{j=1}^{3}x_jz_j) \\
                                & = (x^Tz)^2
\end {aligned}</script><p>不难发觉映射后 9 个特征之间的内积就等于原先 3 个特征间的内积的平方。其实等式最后的 <script type="math/tex">(x^Tz)^2</script> 就是 <strong>核函数</strong> 中的一种。对于有 <script type="math/tex">n</script> 个特征的情况 <strong>核函数</strong> <script type="math/tex">K(x,z)=(x^Tz)^2</script> 的计算值等于特征映射为 <script type="math/tex">n^2</script> 个时的内积值。对于原本需要计算 <script type="math/tex">n^2</script> 次的内积，通过 <strong>核函数</strong> 只需要计算 <script type="math/tex">n</script> 次。更一般的，<strong>核函数</strong> <script type="math/tex">K(x,z)=(x^Tz+c)^d=\langle \phi(x),\phi(z) \rangle</script> 相当于把特征映射到 <script type="math/tex">{n+d \choose d}</script> 维。<strong>核函数</strong> 中有个非常有意思的 <strong>高斯核</strong>，它相当于把特征映射到无限维。</p>
<script type="math/tex; mode=display">
K(x,z)=exp\left(-\frac{||x-z||^2}{2\sigma^2}\right)</script><p>所以现在可以理解 <strong>核函数</strong> 的意义了。由于在 <strong>对偶问题</strong> 中只涉及特征内积的计算，而 <strong>核函数</strong> 在低维计算的值等于特征映射到高维后的内积值，因此我们能够获得相当高效的方法来求解我们的问题。（ 关于 <strong>核函数</strong> 更详细的讨论请看 C299 的讲义 ）</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>简单说说 <strong>SVM</strong> 的正则化，为了解决数据集线性不可分的情况，我们适当调整原问题，使得间距可以比 1 小，但相应的在优化目标中加入一定的代价。</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
min_{\gamma,w,b} & \frac{1}{2}||w||^2+C\sum\limits_{i=1}^{m}\xi_i \\
s.t. & y^{(i)}(w^Tx^{(i)}+b)\ge1-\xi_i, & i=1,...,m \\
 & \xi_i\ge0, & i=1,...,m
\end{array}</script><p>同样使用 <strong>拉格朗日对偶</strong> 可以找到它的对应问题，这个问题也就是 <strong>SVM</strong> 算法所要解决的问题：</p>
<script type="math/tex; mode=display">
\begin{array}{rl} 
max_{\alpha} & W(\alpha)=\sum\limits_{i=1}^{m}\alpha_i - \frac{1}{2} \sum\limits_{i,j=1}^{m}y^{(i)} y^{(j)} \alpha_i \alpha_j \color{red}{\langle x^{(i)},x^{(j)}\rangle} \\
s.t. & 0\le\alpha_i \le C, \ \ \ \ \ i=1,...,m \\
 & \sum\limits_{i=1}^{m}\alpha_iy^{(i)} = 0
\end{array}</script><h3 id="SMO-算法"><a href="#SMO-算法" class="headerlink" title="SMO 算法"></a>SMO 算法</h3><p>这里介绍一下 <strong>SMO</strong> 的主要想法，算法的具体细节可以参考论文。对于没有约束条件的优化问题 <script type="math/tex">\max\limits_{\alpha} W(\alpha_1,\alpha_2,...,\alpha_m)</script>，我们可以依次选取 <script type="math/tex">\alpha_i</script> 并固定其它变量来取得函数的最值，不断这样重复直到函数的值收敛（ 如下图 ）。<br><img src="/2018/05/13/coursera-ml-andrewng-svm/coordinate_ascent.jpeg" title="[coordinate ascent]"><br>对于 <strong>SVM</strong> 中的 <strong>对偶</strong> 问题，思路差不多，每次选取两个变量 <script type="math/tex">\alpha_i,\alpha_j</script>，固定其它的变量。由于要满足一定的约束条件，<script type="math/tex">\alpha_i</script> 可以由 <script type="math/tex">\alpha_j</script> 表示 <script type="math/tex">\alpha_i=(\zeta-\alpha_j y^{(j)})y^{(i)}</script>。从而优化目标可以化为一个关于 <script type="math/tex">\alpha_j</script> 的二次多项式 <script type="math/tex">a\alpha_j^2+b\alpha_j+c</script> 求得最值。当然这里还需要考虑 <script type="math/tex">\alpha_j</script> 的约束范围，对结果进行剪裁。最后不断重复这个过程直到优化目标收敛。</p>
<p>So~，第六周的内容就是这些了，谢谢大家耐心阅读。</p>
<p>P.S. 这周的内容压缩的有些严重，建议大家好好阅读下 CSS229 的讲义。随着 Andrew Ng 的课程即将接近尾声，我准备开始做一些小项目练练手，毕竟能够学以致用才是真正的学习。对于平时看到啥有意思的东西，有啥有意思的想法都会发在 <a href="https://weibo.com/hertzcat" target="_blank" rel="noopener"><strong>微博</strong></a> 上，欢迎大家一同学习 (●—●)~</p>

          <br>
<p>hertzcat</p>
<p class='date'>2018-05-13</p>

        </section>
      </div>
      
        <div class='block'>
  <div id='disqus_thread'></div>
  <script>
    var disqus_shortname = 'hertzcat';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href='http://disqus.com/?ref_noscript'>comments powered by Disqus.</a></noscript>
</div>

      
      <div class='block'>
  
    <a class='main' href='/'>Blog</a>
  
    <a class='main' href='https://github.com/hertzcat'>Github</a>
  
</div>

    </div>
    <footer>
  <p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>
  <span class='muted'>&copy; hertzcat. All Rights Reserved.</span><br>
  <a href='https://github.com/saintwinkle/hexo-theme-scribble' class='muted'>built with Hexo using Scribble theme</a>
  <br>
</footer>

  </body>
</html>
