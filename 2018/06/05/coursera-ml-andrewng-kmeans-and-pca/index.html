<html>
  <head>
    <title>吴恩达机器学习：无监督学习 | k-means 与 PCA - 算力喵</title>
    <link href='/images/fav.jpg' rel='shortcut icon'>
<link href='/atom.xml' rel='alternate' type='application/rss+xml'>
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight.css">
<link rel="stylesheet" href="/css/responsive.css">
<link rel="stylesheet" href="/css/iconfont.css">
<script src="/js/jquery.js"></script>
<script src="/js/basics.js"></script>
<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type'>

  <meta name="keywords" content="吴恩达,Coursera,无监督学习,Python,聚类,k-means,PCA,机器学习,Machine Learning,算力喵">


  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-115469277-1');
  ga('send', 'pageview');
</script>



  <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  </head>
  <body>
    <header>
  <a id='go-back-home' href='/'><img class='avatar' src='/images/scribble.jpg' alt='Home'></a>
  <p id='title'>算力喵</p>
  <p>机器学习不能停~</p>
  <br>
  
  <div class="social-links">
    
      
        <a href="https://weibo.com/hertzcat" class="iconfont icon-weibo" title="weibo"></a>
      
    
      
        <a href="https://www.zhihu.com/people/hertzcat" class="iconfont icon-zhihu" title="zhihu"></a>
      
    
      
        <a href="https://twitter.com/hertzcat0" class="iconfont icon-twitter" title="twitter"></a>
      
    
  </div>
  
</header>

    <div id='container'>
      <div class='block'>
  
    <a class='main' href='/'>Blog</a>
  
    <a class='main' href='https://github.com/hertzcat'>Github</a>
  
</div>

      <section class='paging'>
  
    <div class='left'>
      <a href='/2018/06/13/non-player-game/'>
        ‹
      </a>
    </div>
  
  
    <div class='right'>
      <a href='/2018/05/13/coursera-ml-andrewng-svm/'>
        ›
      </a>
    </div>
  
</section>

      <div class='content'>
        <section class='post'>
          <h1>
            <!-- <div class='date'>2018-06-05</div> -->
            吴恩达机器学习：无监督学习 | k-means 与 PCA
          </h1>
          <p><a href="https://movie.douban.com/subject/26887174/" target="_blank" rel="noopener"><strong>西部世界 第二季</strong></a>刚好放到第七集，课程总结也刚好是第七次。我们关于 <strong>监督学习</strong> 的课程已经告一段落，这次 Ng 将给我们介绍两个很常用的 <strong>无监督学习</strong> 算法。一个是用来将数据划分到不同类别的 <strong>k-means</strong> 算法，一个是用来提取重要特征并给特征降维的 <strong>PCA</strong> 算法。</p>
<p>点击 <a href="https://www.bilibili.com/video/av9912938?from=search&amp;seid=5092317489110083757" target="_blank" rel="noopener"><strong>课程视频</strong></a> 你就能不间断地学习 Ng 的课程，关于课程作业的 Python 代码已经放到了 Github 上，点击 <a href="https://github.com/hertzcat/Coursera-Machine-Learning" target="_blank" rel="noopener"><strong>课程代码</strong></a> 就能去 Github 查看（ 无法访问 Github 的话可以点击 <a href="https://coding.net/u/hertzcat/p/Coursera-Machine-Learning/git?public=true" target="_blank" rel="noopener"><strong>Coding</strong></a> 查看 ），代码中的错误和改进欢迎大家指出。</p>
<p>以下是 Ng 机器学习课程第七周的笔记。</p>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>我们经常会给事物做分类，把特征相近的东西归为一类。而 <strong>无监督学习</strong> 中的聚类正是想让计算机来完成这个工作。用数学语言表达就是要把每个特征数据 <script type="math/tex">x^{(i)}</script> 分配到 <strong>簇</strong> <script type="math/tex">c^{(i)}</script> 中。让我们看下课程作业中的聚类任务来有个直观的感受。（ 下图是将数据分为三个簇 ）<br><img src="/2018/06/05/coursera-ml-andrewng-kmeans-and-pca/k-means.jpg" title="[k-means]"></p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>假设要将数据分到 <script type="math/tex">k</script> 个 <strong>簇</strong>，选取的中心分别为 <script type="math/tex">\mu_1,\mu_2,...,\mu_k</script>。我们的代价函数就是要将每个特征与被分配到的簇的中心之间的距离和最小化（ 直观来说就是凑得近的分一起 ）。</p>
<script type="math/tex; mode=display">
J(c^{(1)},c^{(2)},...,c^{(m)};\mu_1,\mu_2,...,\mu_k)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_{c^{(i)}}||^2</script><p>可以看出 <strong>代价函数</strong> 一方面取决于 <script type="math/tex">x^{(i)}</script> 的分配，一方面取决于 <strong>簇</strong> 中心的位置。</p>
<h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>下面我们先来看下 <strong>k-means</strong> 的算法，然后来解释它是如何优化 <strong>代价函数</strong> 的。</p>
<blockquote>
<p>随机选择 <script type="math/tex">k</script> 个 <strong>簇</strong> 的中心 <script type="math/tex">\mu_1,\mu_2,...,\mu_k</script><br>重复下面步骤直到收敛：</p>
<ol>
<li>对于每个 <script type="math/tex">x^{(i)}</script>，计算 <script type="math/tex">c^{(i)}</script> （ <script type="math/tex">x^{(i)}</script> 距离第 <script type="math/tex">j</script> 个簇的中心最近，则 <script type="math/tex">c^{(i)}=j</script> ）</li>
<li>更新 <script type="math/tex">\mu_k</script> （ 新 <script type="math/tex">\mu_k</script> 为所有满足 <script type="math/tex">c^{(i)}=k</script> 的 <script type="math/tex">x^{(i)}</script> 的中心 ）</li>
</ol>
</blockquote>
<p>对于步骤 <script type="math/tex">1</script> 来说，我们固定了 <script type="math/tex">\mu_1,\mu_2,...,\mu_k</script>，并为每个 <script type="math/tex">x^{(i)}</script> 选取了距离最近的 <strong>簇</strong>，这使得 <strong>代价函数</strong> 减小。对于步骤 <script type="math/tex">2</script>，我们固定了 <script type="math/tex">c^{(1)},c^{(2)},...,c^{(m)}</script>，并将 <script type="math/tex">\mu_k</script> 移动到了各个分类的中心，这也使得 <strong>代价函数</strong> 减小。因此随着不断循环这个过程，我们将得到一个最优解（ 也可能是局部最优 ）。</p>
<h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><p><strong>k-means</strong> 算法挺好理解的，最后我们来说说簇中心的初始化与 <script type="math/tex">k</script> 的选择问题。对于簇中心的初始化，一般直接随机选 <script type="math/tex">k</script> 个数据为中心，选比如说 100 次并计算 <strong>代价函数</strong> 的值，选择其中最小的那一次的结果。<br>对于 <script type="math/tex">k</script> 的选择，一方面可以运用 <strong>肘部法则</strong>，原理是 <strong>代价函数</strong> 一开始会随着 <script type="math/tex">k</script> 的增大下降的很快，但过了某个值之后下降变缓，我们选择这个点的 <script type="math/tex">k</script> 值。另一方面我们可以根据自己的业务需求来选择 <script type="math/tex">k</script>（ 业务就是需要分为 <script type="math/tex">k</script> 个类的情况 ）。</p>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>在机器学习中，降低特征的维度能够给我们带来很多好处。首先降低特征的维度可以提升学习的效率。其次降维可以让我们的注意力集中在重要的特征上。再有降维也可以作为一种数据压缩的方法。这里将要学习的降维算法被称作 <strong>主成分分析</strong>（ <strong>PCA</strong> ）。</p>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>在统计学中，<strong>协方差</strong> 表现了两组数据之间的关联性（ 当数据为同一个是就是 <strong>方差</strong> ）。为了了解 <strong>协方差</strong> 与降维算法的关系，我们先来看简单的二维情况。假设我们的数据有两个特征（ 如图 ），并且已经做过 <strong>标准化</strong>。红色的数据点在这个坐标系下呈现出明显的先关性（ <strong>协方差</strong> 大 ）。<br><img src="/2018/06/05/coursera-ml-andrewng-kmeans-and-pca/covariance.jpg" title="[基]"><br>由 <strong>线性代数</strong> 的知识可以知道，我们可以使用不同的 <strong>基</strong> （ 特征 ）来表示这些数据，如图中的绿色坐标和蓝色坐标。通过直觉可以发现如果将数据投影到较长的蓝色坐标上来降维是更为合适的选择。事实上这也是使得 <strong>协方差</strong> 最小的选择，<strong>协方差</strong> 的最小化让每个特征和其它特征的关系性降到最低，使得每个特征被独立地表达，而降维就是从中选出贡献（ <strong>方差</strong> ）比较大的特征。对于数据 <script type="math/tex">X</script>（ 这里的 <script type="math/tex">X</script> 和课程作业中的不同，每列表示一个数据 ），特征间的 <strong>协方差</strong> 可以表示为 <strong>协方差矩阵</strong>。</p>
<script type="math/tex; mode=display">
C = \frac{1}{m} XX^{T} = \frac{1}{m} \begin{bmatrix}
    \sum\limits_{i=1}^{m}(x_1^{(i)})^2 & \sum\limits_{i=1}^{m}(x_1^{(i)}x_2^{(i)}) \\
    \sum\limits_{i=1}^{m}(x_2^{(i)}x_1^{(i)}) & \sum\limits_{i=1}^{m}(x_2^{(i)})^2 \\
\end{bmatrix}</script><p><strong>协方差矩阵</strong> 对角线上的元素是数据在各个特征上的 <strong>方差</strong>，其余是特征之间的 <strong>协方差</strong>。于是降维的目标就成了找到一个基变换让 <strong>协方差矩阵</strong> 里除了对角线上的值，其余的都尽可能小。不过在下面的讨论中，我们会知道可以找到一组 <strong>基</strong> 使得变换后的 <strong>协方差</strong> 除了对角线上的值，其余都为 0。</p>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>假设我们的数据 <script type="math/tex">X</script> 通过基变换映射到了 <script type="math/tex">Y</script>，<script type="math/tex">Y=PX</script>，<script type="math/tex">Y</script> 的 <strong>协方差</strong> <script type="math/tex">D</script> 为：</p>
<script type="math/tex; mode=display">
\begin{array}{l l l}
  D & = & \frac{1}{m}YY^{T} \\
    & = & \frac{1}{m}(PX)(PX)^{T} \\
    & = & \frac{1}{m}PXX^{T}P^{T} \\
    & = & P(\frac{1}{m}XX^{T})P^{T} \\
    & = & PCP^{T}
\end{array}</script><p>到这里就可以看出我们能够利用 <strong>矩阵对角化</strong> 的方法来得到 <script type="math/tex">D</script>，就是存在单位正交特征向量 <script type="math/tex">E</script>，使得 <script type="math/tex">E^{T}CE=\Lambda</script>，其中 <script type="math/tex">\Lambda</script> 为 <strong>对角矩阵</strong>。通过这种方法可以得到基 <script type="math/tex">P=E^{T}</script>，而 <script type="math/tex">\Lambda</script> 对角上的元素表示各个特征的贡献大小。不过我们还可以通过 <strong>矩阵分解</strong> 的技巧来解决这个问题，使用 <strong>SVD</strong> 来分解矩阵的话有 <script type="math/tex">X=U \Sigma V^T</script>，经过推导可以得到：</p>
<script type="math/tex; mode=display">
\begin{array}{l l l}
  XX^{T} & = & (U \Sigma V^T)(U \Sigma V^T)^T \\
    & = & (U \Sigma V^T)(V \Sigma U^T) \\
    & = & U \Sigma^2 U^T
\end{array}</script><p>所以可以取 <script type="math/tex">P=U^{T}</script>，而 <script type="math/tex">D=\frac{1}{m} \Sigma^2</script>。对于降维，只要取 <script type="math/tex">D</script> 中占比比较高的特征对应的 <strong>基</strong>，也就是 <script type="math/tex">P</script> 的一部分 <script type="math/tex">P'</script>，对应的数据也变为了 <script type="math/tex">Y'</script>。对于数据恢复，可以通过简单的推导得到 <script type="math/tex">X_{恢复}=P'^TY'</script>。</p>
<h3 id="课外阅读"><a href="#课外阅读" class="headerlink" title="课外阅读"></a>课外阅读</h3><p>关于 <strong>PCA</strong> 的内容，我觉得有两篇文章对我很有帮助，在这里分享给大家：<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener"><strong>PCA的数学原理</strong></a>、<a href="https://liam0205.me/2017/11/22/SVD-for-Human-Beings/" target="_blank" rel="noopener"><strong>谈谈矩阵的 SVD 分解</strong></a>。</p>
<p>So~，第七周的内容就是这些了，谢谢大家耐心阅读。</p>

          <br>
<p>hertzcat</p>
<p class='date'>2018-06-05</p>

        </section>
      </div>
      
        <div class='block'>
  <div id='disqus_thread'></div>
  <script>
    var disqus_shortname = 'hertzcat';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href='http://disqus.com/?ref_noscript'>comments powered by Disqus.</a></noscript>
</div>

      
      <div class='block'>
  
    <a class='main' href='/'>Blog</a>
  
    <a class='main' href='https://github.com/hertzcat'>Github</a>
  
</div>

    </div>
    <footer>
  <p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>
  <span class='muted'>&copy; hertzcat. All Rights Reserved.</span><br>
  <a href='https://github.com/saintwinkle/hexo-theme-scribble' class='muted'>built with Hexo using Scribble theme</a>
  <br>
</footer>

  </body>
</html>
